#
# Parameters to start a simulation with a trader
# @renero
#

# Debug flag
debug: true

# Data Path. Where is the data from which l#earning how to invest?
data_path: ../output/forecast_acciona_2018b.csv
delimiter: ','

# Wanna save the model after training?
save_model: true
models_dir: ../output/

# Specify here if you want to load a pre-trained model
load_model: true
model_file: ../output/rl_model_forecast_acciona_2018b_2.json
weights_file: ../output/rl_model_forecast_acciona_2018b_2.h5

# Parameters for the Q-Learning function
gamma: 0.95
epsilon: 1.0
epsilon_min: 0.02
decay_factor: 0.995

# When to perform learning
train_steps: 32
start_steps: 100

# Batch size for training the model
batch_size: 32

# Experience Replay batch size
experience_replay: False
exp_batch_size: 16

# TensorBoard?
tensorboard: False
tbdir: ./graphs/

# Reduction factor is the amount by which the number of cells in the network
# is to be reduced. If 1.0, then the num of cells will be the number of
# possible states times the number of actions.
cells_reduction_factor: 0.25

# Num. of episodes to run simulate and learn
# Every how many episodes display the update in q-learning?
num_episodes: 300
num_episodes_update: 10

# How many previous states of the environment to store?
stack_size: 3

# Different states in which the environment might be.
state:
  Gain:
    names:
      - GAIN
      - LOSE
  HaveShares:
    names:
      - HAVE
      - DONT
  CanBuy:
    names:
      - BUY
      - NOB
  CanSell:
    names:
      - SELL
      - NOS
  PredUpward:
    names:
      - UPW
      - DWN
  LastPredOk:
    names:
      - LOK
      - LNOK
  PrevLastPredOk:
    names:
      - PLOK
      - PLNOK
#  PrevPrevLastPredOk:
#    names:
#      - PPLOK
#      - PPLNOK

# Actions to be accomplished by the agent (Portfolio class)
action:
  - wait
  - buy
  - sell

# Environment variables
environment:
  initial_budget: 100.
  consider_stop_loss: false
  stop_loss: .05
  proportional_reward: false
  reward_do_nothing: -1.
  reward_success_buy: 0.
  reward_positive_sell: +1.
  reward_negative_sell: -0.1
  reward_failed_buy: -1.
  reward_failed_sell: -1.

# Log headers
table_headers:
  - t
  - price
  - forecast
  - budget
  - investment
  - value
  - netValue
  - shares
  - action
  - reward
  - state

# Seed to be used
seed: 25

# Network parameters
deep_qnet:
  hidden_layers:
    - 64
    - 32
    - 8
  activation: relu
  loss: mse
  optimizer: adam
  metrics:
    - mae
