#
# Parameters to start a simulation with a trader
# @renero
#

# Debug flag
# Log level:
# 0 = Silent, 1 = Errors only, 2 = Warnings too, 3 = info, 4 = Debug
debug: true
log_level: 3

# Data Path. Where is the data from which l#earning how to invest?
data_path: ../output/forecast_acciona_konkorde_2018b.csv
delimiter: ','
column_name:
  price: test_y
  forecast: forecast
  green: verde
  blue: azul

# Wanna save the model after training?
save_model: false
models_dir: ../output/

# Specify here if you want to load a pre-trained model
model_file: ../output/rl_model_forecast_acciona_konkorde_2010_2018

# Parameters for the Q-Learning function
gamma: 0.95
epsilon: 1.0
epsilon_min: 0.02
decay_factor: 0.995

# When to perform learning
train_steps: 24
start_episodes: 10

# Batch size for training the model
batch_size: 32

# Experience Replay batch size
experience_replay: False
exp_batch_size: 16

# Konkorde threshold, above which we can consider an upward trend
k_threshold: 0.15

# TensorBoard?
tensorboard: False
tbdir: ./graphs/

# Reduction factor is the amount by which the number of cells in the network
# is to be reduced. If 1.0, then the num of cells will be the number of
# possible states times the number of actions.
cells_reduction_factor: 0.25

# Num. of episodes to run simulate and learn
# Every how many episodes display the update in q-learning?
num_episodes: 300
num_episodes_update: 10

# How many previous states of the environment to store?
stack_size: 3

# Different states in which the environment might be.
state:
  Gain:
    names:
      - GAIN
      - LOSE
  HaveShares:
    names:
      - HAVE
      - DONT
  CanBuy:
    names:
      - BUY
      - NOB
  CanSell:
    names:
      - SELL
      - NOS
  PredUpward:
    names:
      - UPW
      - DWN
  LastPredOk:
    names:
      - LOK
      - LNOK
  PrevLastPredOk:
    names:
      - PLOK
      - PLNOK
  PrevPrevLastPredOk:
    names:
      - PPLOK
      - PPLNOK
  Konkorde:
    names:
      - UPTREND
      - DOWNTREND

# Actions to be accomplished by the agent (Portfolio class)
action:
  - wait
  - buy
  - sell

# Environment variables
environment:
  initial_budget: 100.
  consider_stop_loss: false
  stop_loss: .05
  direct_reward: true         # If True, reward is always netValue, not
                              # considering the values below.
  proportional_reward: false  # Controls if reward is multiplied by net value.
                              # when selling shares. Incompatible with
                              # direct_reward
  reward_do_nothing: -1.      # -0.94
  reward_success_buy: 0.      # -0.94
  reward_positive_sell: +1.   # +0.45
  reward_negative_sell: -0.1  # -0.24
  reward_failed_buy: -1.      # +0.99
  reward_failed_sell: -1.     # -0.45

# Log headers
table_headers:
  - t
  - price
  - forecast
  - budget
  - investment
  - value
  - netValue
  - shares
  - action
  - reward
  - state

# Seed to be used
seed: 25

# Network parameters
deep_qnet:
  hidden_layers:
    - 64
    - 32
    - 8
  activation: relu
  loss: mse
  optimizer: adam
  metrics:
    - mae
